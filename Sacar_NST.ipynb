{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ky2oJ5otwJrx","BJ1xcYFJCob7","fGDsNSgGndde","1J659Zpm3Uf9","-0mSGuMfvrg2"],"authorship_tag":"ABX9TyOlQ6ShOuwmMWgFhSAAkYjD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ky2oJ5otwJrx"},"source":["# Importación de librerias"]},{"cell_type":"code","source":["!pip install -U kaleido"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRC8qTBVm5PU","executionInfo":{"status":"ok","timestamp":1676110489723,"user_tz":-60,"elapsed":18093,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}},"outputId":"54c3ba39-0611-4d5f-def2-3c233e4aab0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.2.1\n"]}]},{"cell_type":"code","metadata":{"id":"qToRUIq3wTCP","executionInfo":{"status":"ok","timestamp":1676110522548,"user_tz":-60,"elapsed":32842,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4703a4e2-4a1c-460a-9aec-fa1896a852de"},"source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import os\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as px\n","\n","import PIL.Image\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"BJ1xcYFJCob7"},"source":["# Creación de variables y fuciones"]},{"cell_type":"markdown","source":["## Ajustes NST"],"metadata":{"id":"fGDsNSgGndde"}},{"cell_type":"code","metadata":{"id":"pL432xyQc3us"},"source":["STYLE_WEIGHT = 100\n","CONTENT_WEIGHT = 0.5\n","Nepochs = 20000\n","muestra_cada = 200\n","Imsize = 286"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOWdTOc3Cob8"},"source":["def vgg_layers(layer_names):\n","    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n","    # Load our model. Load pretrained VGG, trained on imagenet data\n","    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","    vgg.trainable = False\n","\n","    outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","    model = tf.keras.Model([vgg.input], outputs)\n","    return model\n","  \n","\n","def gram_matrix(input_tensor):\n","    result = tf.math.reduce_mean(input_tensor, [1,2])\n","    #input_shape = tf.shape(input_tensor)\n","    #num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n","    return result#/(num_locations)\n","\n","# Extraer estilo y contenido\n","class StyleContentModel(tf.keras.models.Model):\n","    def __init__(self, style_layers, content_layers):\n","        super(StyleContentModel, self).__init__()\n","        self.vgg = vgg_layers(style_layers + content_layers)\n","        self.style_layers = style_layers\n","        self.content_layers = content_layers\n","        self.num_style_layers = len(style_layers)\n","        self.vgg.trainable = False\n","\n","    def call(self, inputs):\n","        \"Expects float input in [0,1]\"\n","        inputs = inputs*255.0\n","        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","        outputs = self.vgg(preprocessed_input)\n","        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n","                                          outputs[self.num_style_layers:])\n","\n","        style_outputs = [gram_matrix(style_output)\n","                         for style_output in style_outputs]\n","\n","        content_dict = {content_name: value\n","                        for content_name, value\n","                        in zip(self.content_layers, content_outputs)}\n","\n","        style_dict = {style_name: value\n","                      for style_name, value\n","                      in zip(self.style_layers, style_outputs)}\n","\n","        return {'content': content_dict, 'style': style_dict}\n","\n","\n","# Función de pérdidas\n","def style_content_loss(outputs,style_weight,content_weight):\n","    style_outputs = outputs['style']\n","    content_outputs = outputs['content']\n","\n","    style_loss = [style_weight*tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","                           for name in style_outputs.keys()]\n","\n","    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n","                             for name in content_outputs.keys()])\n","    content_loss *= content_weight \n","    loss = style_loss + content_loss\n","    \n","    return loss, style_loss, content_loss\n","\n","\n","def clip_0_1(image):\n","      return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6BqAdpMM1Sv"},"source":["opt = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.99, epsilon=1e-1)\n","\n","style_layers_ori = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1']\n","\n","content_layers = ['block5_conv2'] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ajustes visualización"],"metadata":{"id":"1J659Zpm3Uf9"}},{"cell_type":"code","source":["def one_hot_mask(y):\n","    ''' Do the one hot encoding for the masks.\n","  \n","    Arguments:\n","        - y (tf tensor): Mask of shape (height, width, 3)\n","\n","    Returns:\n","        - mask (tf tensor): Mask after do the one hot. Shape (height, width, num_classes) '''\n","\n","    one_hot_map = []\n","    for color in colors:\n","        class_map = tf.reduce_all(tf.equal(y, color), axis = -1)\n","        one_hot_map.append(class_map)\n","    mask = tf.cast(tf.stack(one_hot_map, axis = -1), tf.int32)\n","    return mask\n","\n","\n","def load_image(folder, file, height = 96, width = 256, crop = False):\n","    ''' Load and preprocess a train image by:\n","        - Crop the image to not have the Mercedes-Benz star\n","        - Resize the image to (height, width)\n","        - Normalize the image to [0, 1]\n","  \n","    Arguments:\n","        - folder (string): Path to the folder\n","        - file (string): Name of the file to load\n","        - height (int): Height to resize -- 96\n","        - width (int): Width to resize -- 256\n","        - crop (bool): Crpo the image or not -- True\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed image '''\n","\n","    # Load the image (png)\n","    image = tf.io.read_file(folder + '/' + file)\n","    image = tf.cast(tf.image.decode_png(image, channels = 3), tf.float32)\n","\n","    # Crop the image\n","    if crop:\n","        image = tf.image.crop_to_bounding_box(image, 0, 0, 768, 2048)\n","\n","    # Resize the image\n","    image = tf.image.resize(image, (height, width))\n","\n","    # Normalize the image\n","    image = tf.cast(image, tf.float32)/255.0\n","    return image\n","\n","\n","def load_mask(folder, file, height = 96, width = 256, one_hot = True, crop = False):\n","    ''' Load and preprocess a train mask by:\n","        - Crop the image to not have the Mercedes-Benz star\n","        - Resize the image to (height, width)\n","        - Reshaping the mask from (height, width, 3) to (height, width, 30): One hot encoding\n","  \n","    Arguments:\n","        - folder (string): Path to the folder\n","        - file (string): Name of the file to load\n","        - height (int): Height to resize -- 96\n","        - width (int): Width to resize -- 256\n","        - one_hot (bool): Do one hot encoding or not -- True\n","        - crop (bool): Crpo the image or not -- True\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed mask '''\n","\n","    # Load the mask (png)\n","    image = tf.io.read_file(folder + '/' + file)\n","    image = tf.cast(tf.image.decode_png(image, channels = 3), tf.int32)\n","\n","    # Crop the mask\n","    if crop:\n","        image = tf.image.crop_to_bounding_box(image, 0, 0, 768, 2048)\n","\n","    # Resize the mask\n","    image = tf.image.resize(image, (height, width), method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","    # One hot encoding\n","    if one_hot:\n","        image = one_hot_mask(image)\n","    return image\n","\n","\n","def one_hot_to_color_mask(mask, colors, img_height = 96, img_width = 256):\n","    ''' Convert from the mask from the classes with highest probablities to the correct color. From (96, 256, 1) to (96, 256, 3).\n","  \n","    Arguments:\n","        - mask (tf tensor): Mask with the classes with highest probabilities\n","        - colors (list): List with the class colors\n","        - img_height (int): Height of the images -- 96\n","        - img_width (int): Width of the images -- 256\n","\n","    Returns:\n","        - color_mask (tf tensor): Color mask '''\n","\n","    color_mask = np.zeros((img_height, img_width, channels)).astype('float')\n","    for c in range(len(colors)):\n","        color_true = mask == c\n","        for i in range(3):\n","            color_mask[:,:,i] += color_true*colors[c][i]\n","\n","    color_mask = tf.cast(color_mask, dtype = tf.int32)\n","\n","    return color_mask\n","\n","def load_train(image_name, mask_name):\n","    ''' Load and preprocess a train image and its mask\n","  \n","    Arguments:\n","        - image_name (string): Name of the image to load\n","        - mask_name (string): Name of the mask to load\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed image\n","        - mask (tf tensor): Preprocessed mask '''\n","\n","    image = load_image(train_images_folder_path, image_name, img_height, img_width)\n","    mask = load_mask(train_mask_folder_path, mask_name, img_height, img_width)\n","    return image, mask"],"metadata":{"id":"sEahSJGk2vLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_images_folder_path = \"/content/drive/My Drive\" \n","train_mask_folder_path = \"/content/drive/MyDrive/GTA Segmentacion\" \n","\n","img_height, img_width, channels = 96, 256, 3\n","\n","batch_size = 2\n","\n","colors = np.array([(0, 0, 0), (111, 74, 0), (81, 0, 81), (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153), (180, 165, 180), \n","                   (150, 100, 100), (150, 120, 90), (153, 153, 153), (250, 170, 30), (220, 220, 0), (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), ( 0, 0, 142), \n","                   ( 0, 0, 70), (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)], dtype = np.int32)\n","\n","model = tf.keras.models.load_model('/content/drive/My Drive/Modelo_segmentacion_UNET/best_model_weights_and_architecture')"],"metadata":{"id":"n9jfJ4Ce2vJt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inicialización"],"metadata":{"id":"-0mSGuMfvrg2"}},{"cell_type":"code","source":["path_result = \"/content/drive/MyDrive/Resultados/\"\n","\n","path_GTA = \"/content/drive/MyDrive/GTA/\"\n","path_Reales = \"/content/drive/MyDrive/Reales/\"\n","\n","itera = 60  # Número de imagenes de prueba\n","\n","list_GTA = os.listdir(path_GTA)\n","images_GTA = random.choices(list_GTA, k=itera)\n","\n","list_Reales = os.listdir(path_Reales)\n","images_Reales = random.choices(list_Reales, k=itera)\n","\n","for i in range(itera):\n","  images_GTA[i] = path_GTA + images_GTA[i]\n","  images_Reales[i] = path_Reales + images_Reales[i]"],"metadata":{"id":"xhO1MBPfXoty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cambiar alguna imagen si hace falta"],"metadata":{"id":"bmXdVwsQeAZQ"}},{"cell_type":"code","source":["a = [\"01526.png\", \"00031.png\", \"00513.png\", \"01166.png\", \"01151.png\", \"01782.png\", \"01550.png\", \"01450.png\", \"00557.png\", \"02078.png\",  # 10 primeros\n","     \"01575.png\", \"00370.png\", \"01401.png\", \"01999.png\", \"00203.png\", \"00560.png\", \"00142.png\", \"02074.png\", \"02406.png\", \"00467.png\",\n","     \"01889.png\", \"00466.png\", \"02307.png\", \"00529.png\", \"02071.png\", \"00045.png\", \"00301.png\", \"00491.png\", \"00078.png\", \"01289.png\",\n","     \"01862.png\", \"02264.png\", \"01297.png\", \"01385.png\", \"01348.png\", \"02019.png\", \"02099.png\", \"00242.png\", \"01101.png\", \"02313.png\",\n","     \"01655.png\", \"00923.png\", \"01495.png\", \"01947.png\", \"02404.png\", \"01293.png\", \"00733.png\", \"01579.png\", \"01701.png\", \"00248.png\", \n","     \"01597.png\", \"01665.png\", \"01909.png\", \"00857.png\", \"00646.png\", \"02134.png\", \"00993.png\", \"01394.png\", \"01600.png\", \"01012.png\"]\n","\n","b = [\"bremen_000152_000019_leftImg8bit.png\", \"stuttgart_000115_000019_leftImg8bit.png\", \"dusseldorf_000141_000019_leftImg8bit.png\",\n","      \"monchengladbach_000000_007098_leftImg8bit.png\", \"cologne_000006_000019_leftImg8bit.png\", \"bremen_000256_000019_leftImg8bit.png\",\n","      \"bremen_000190_000019_leftImg8bit.png\", \"cologne_000072_000019_leftImg8bit.png\", \"cologne_000064_000019_leftImg8bit.png\", \"strasbourg_000000_029020_leftImg8bit.png\", # 10 primeros\n","      \"aachen_000137_000019_leftImg8bit.png\", \"dusseldorf_000030_000019_leftImg8bit.png\", \"bremen_000004_000019_leftImg8bit.png\", \"bremen_000085_000019_leftImg8bit.png\",\n","      \"dusseldorf_000026_000019_leftImg8bit.png\", \"bochum_000000_009951_leftImg8bit.png\", \"tubingen_000120_000019_leftImg8bit.png\", \"hamburg_000000_032906_leftImg8bit.png\",\n","      \"hanover_000000_055800_leftImg8bit.png\", \"monchengladbach_000000_033454_leftImg8bit.png\", \"stuttgart_000065_000019_leftImg8bit.png\", \"bremen_000183_000019_leftImg8bit.png\",\n","      \"bremen_000079_000019_leftImg8bit.png\", \"stuttgart_000087_000019_leftImg8bit.png\", \"tubingen_000035_000019_leftImg8bit.png\", \"strasbourg_000001_022151_leftImg8bit.png\",\n","      \"aachen_000140_000019_leftImg8bit.png\", \"cologne_000094_000019_leftImg8bit.png\", \"jena_000052_000019_leftImg8bit.png\", \"tubingen_000059_000019_leftImg8bit.png\",\n","      \"aachen_000009_000019_leftImg8bit.png\", \"darmstadt_000074_000019_leftImg8bit.png\", \"strasbourg_000000_013863_leftImg8bit.png\", \"aachen_000097_000019_leftImg8bit.png\",\n","      \"dusseldorf_000033_000019_leftImg8bit.png\", \"cologne_000145_000019_leftImg8bit.png\", \"krefeld_000000_021222_leftImg8bit.png\", \"hamburg_000000_078407_leftImg8bit.png\",\n","      \"hamburg_000000_053486_leftImg8bit.png\", \"hanover_000000_026804_leftImg8bit.png\", \"bremen_000121_000019_leftImg8bit.png\", \"stuttgart_000012_000019_leftImg8bit.png\",\n","      \"bremen_000135_000019_leftImg8bit.png\", \"jena_000070_000019_leftImg8bit.png\", \"bochum_000000_023040_leftImg8bit.png\", \"jena_000108_000019_leftImg8bit.png\",\n","      \"stuttgart_000155_000019_leftImg8bit.png\", \"bochum_000000_030913_leftImg8bit.png\", \"strasbourg_000001_031272_leftImg8bit.png\", \"tubingen_000075_000019_leftImg8bit.png\",\n","      \"tubingen_000048_000019_leftImg8bit.png\", \"cologne_000005_000019_leftImg8bit.png\", \"dusseldorf_000097_000019_leftImg8bit.png\", \"bremen_000059_000019_leftImg8bit.png\",\n","      \"hamburg_000000_098400_leftImg8bit.png\", \"hamburg_000000_081299_leftImg8bit.png\", \"aachen_000004_000019_leftImg8bit.png\", \"hamburg_000000_079376_leftImg8bit.png\",\n","      \"bremen_000112_000019_leftImg8bit.png\", \"monchengladbach_000000_006518_leftImg8bit.png\"]\n","\n","for i in range(itera):\n","  images_GTA[i] = path_GTA + a[i]\n","  images_Reales[i] = path_Reales + b[i]"],"metadata":{"id":"xrHA4IWGQVVr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Para saber que imágenes que se han escogido"],"metadata":{"id":"OnYK6mokutJY"}},{"cell_type":"code","source":["for name in images_GTA:\n","  print(name.split(\"/\")[-1])\n","print(\"------------------------------------------------\")\n","for name in images_Reales:\n","  print(name.split(\"/\")[-1])"],"metadata":{"id":"z_Z60WCdLij4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Automático para muchos"],"metadata":{"id":"Txyj7llrzZuy"}},{"cell_type":"code","source":["n_style = [50,75,100,200,500,1000]\n","\n","for num in range(0, itera):\n","\n","# NST  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  style_image = PIL.Image.open(images_Reales[num])\n","  content_image = PIL.Image.open(images_GTA[num])\n","\n","  content_image = np.asarray(content_image.resize((Imsize,Imsize)))\n","  style_image = np.asarray(style_image.resize((Imsize,Imsize)))\n","\n","  content_image = content_image[:,:,0:3].astype(np.float32)/255\n","  style_image = style_image[:,:,0:3].astype(np.float32)/255\n","\n","  content_image = np.expand_dims(content_image,0)\n","  style_image = np.expand_dims(style_image,0)\n","\n","\n","  for s in n_style:\n","    STYLE_WEIGHT = s # Cambiamos la cantidad de estilo en cada irteración\n","\n","    for layer in np.arange(1,2):\n","      style_layers = style_layers_ori[0:5]\n","\n","      extractor = StyleContentModel(style_layers, content_layers)\n","      style_targets = extractor(style_image)['style']\n","      content_targets = extractor(content_image)['content']\n","\n","      image = tf.Variable(content_image.copy())\n","\n","      style_weight_FG=5*1e-11\n","      content_weight_FG=1e-4\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(content_image),style_weight_FG,content_weight_FG)\n","      loss_style = np.array(style_loss[0])\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(style_image),style_weight_FG,content_weight_FG)\n","      loss_content = np.array(content_loss)\n","\n","      style_weight= style_weight_FG * STYLE_WEIGHT/loss_style\n","      content_weight= content_weight_FG * CONTENT_WEIGHT/loss_content\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(content_image),style_weight,content_weight)\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(style_image),style_weight,content_weight)\n","\n","      @tf.function()\n","      def train_step(image):\n","          with tf.GradientTape() as tape:\n","              outputs = extractor(image)\n","              loss = style_content_loss(outputs,style_weight,content_weight)\n","\n","          grad = tape.gradient(loss[1][0], image)\n","          opt.apply_gradients([(grad, image)])\n","          image.assign(clip_0_1(image))\n","\n","      L_cont = np.array([])\n","      L_style = np.array([]) \n","\n","      for n in range(0,Nepochs):\n","          train_step(image)\n","          \n","          if np.mod(n,muestra_cada)==0:\n","              outputs = extractor(image)\n","              loss, style_loss, content_loss = style_content_loss(outputs,style_weight,content_weight)\n","              \n","              L_cont = np.append(L_cont,np.array(content_loss))\n","              L_style = np.append(L_style,np.array(style_loss[0]))\n","\n","      # Resultados\n","      plt.figure()\n","      plt.imshow(image[0, :,:,:],vmin=0,vmax=1), plt.axis('off')\n","      plt.savefig(path_result + \"AA_NST_\" + str(s) + \"_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","\n","# Segmentación  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  images_name = [\"GTA/\" + images_GTA[num].split(\"/\")[-1], \"Resultados/AA_NST_50_\" + str(num+1) + \".png\", \"Resultados/AA_NST_75_\" + str(num+1) + \".png\", \"Resultados/AA_NST_100_\" + str(num+1) + \".png\"]\n","\n","  mask_names = ((images_name[0].split(\"/\")[-1] + \" \") * 4).split()\n","\n","  dataset = tf.data.Dataset.from_tensor_slices((images_name, mask_names))\n","  dataset = dataset.map(load_train, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.batch(batch_size)\n","\n","  predictions = model.predict(dataset, batch_size = 10)\n","  predictions = np.argmax(predictions, axis = 3)\n","\n","  idx = range(len(images_name))\n","  visualize_images = [load_image(train_images_folder_path, images_name[i], img_height, img_width) for i in idx]\n","  visualize_masks = [load_mask(train_mask_folder_path, mask_names[i], img_height, img_width, one_hot = False) for i in idx]\n","  preds = predictions[idx]\n","\n","  iou_score = []\n","  iou_score_half = []\n","\n","  plt.figure(figsize = (16, 4))\n","  plt.subplots_adjust(hspace = 0.1)\n","\n","  for k in range(4):\n","    plt.subplot(3, 4, k + 1)\n","    plt.imshow(visualize_images[k])\n","    # if k==0:\n","    #   plt.ylabel(\"Input\")\n","    # plt.title([\"Original\", \"50\", \"75\", \"100\"][k])\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","    plt.subplot(3, 4, k + 5)\n","    plt.imshow(visualize_masks[k])\n","    # if k==0:\n","    #   plt.ylabel(\"Ground Truth\")\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","    output = one_hot_to_color_mask(preds[k], colors)\n","# IoU calculation\n","    intersection = 0\n","    union = img_height * img_width\n","\n","    for i in range(img_height):\n","      for j in range(img_width):\n","        a = visualize_masks[k][i-1,j-1,:] == output[i-1,j-1,:]\n","        if sum(a.numpy())==3:\n","          intersection += 1\n","\n","    iou_score.append(round(intersection/union, ndigits=2))\n","\n","# Calcula de la mitad hacia abajo\n","    intersection = 0\n","    union = int(img_height/2) * img_width\n","\n","    for i in range(int(img_height/2), img_height):\n","        for j in range(img_width):\n","          a = visualize_masks[k][i-1,j-1,:] == output[i-1,j-1,:]\n","          if sum(a.numpy())==3:\n","            intersection += 1\n","\n","    iou_score_half.append(round(intersection/union, ndigits=2))\n","\n","    plt.subplot(3, 4, k + 9)\n","    plt.imshow(output)\n","    # if k==0:\n","    #   plt.ylabel(\"Output\")\n","    plt.xlabel(\"IoU: %s\" % iou_score[k])\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","  plt.savefig(path_result + \"A_NSTs_\" + str(num+1) + \"_1.png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","# Segmentación 2  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  images_name = [\"GTA/\" + images_GTA[num].split(\"/\")[-1], \"Resultados/AA_NST_200_\" + str(num+1) + \".png\", \"Resultados/AA_NST_500_\" + str(num+1) + \".png\", \"Resultados/AA_NST_1000_\" + str(num+1) + \".png\"]\n","\n","  dataset = tf.data.Dataset.from_tensor_slices((images_name, mask_names))\n","  dataset = dataset.map(load_train, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.batch(batch_size)\n","\n","  predictions = model.predict(dataset, batch_size = 10)\n","  predictions = np.argmax(predictions, axis = 3)\n","\n","  visualize_images = [load_image(train_images_folder_path, images_name[i], img_height, img_width) for i in idx]\n","  preds = predictions[idx]\n","\n","  plt.figure(figsize = (16, 4))\n","  plt.subplots_adjust(hspace = 0.1)\n","\n","  for k in range(4):\n","    plt.subplot(3, 4, k + 1)\n","    plt.imshow(visualize_images[k])\n","    # if k==0:\n","    #   plt.ylabel(\"Input\")\n","    # plt.title([\"Original\", \"200\", \"500\", \"1000\"][k])\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","    plt.subplot(3, 4, k + 5)\n","    plt.imshow(visualize_masks[k])\n","    # if k==0:\n","    #   plt.ylabel(\"Ground Truth\")\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","    output = one_hot_to_color_mask(preds[k], colors)\n","# IoU calculation\n","    intersection = 0\n","    union = img_height * img_width\n","\n","    for i in range(img_height):\n","      for j in range(img_width):\n","        a = visualize_masks[k][i-1,j-1,:] == output[i-1,j-1,:]\n","        if sum(a.numpy())==3:\n","          intersection += 1\n","\n","    iou_score.append(round(intersection/union, ndigits=2))\n","    \n","# Calcula de la mitad hacia abajo\n","    intersection = 0\n","    union = int(img_height/2) * img_width\n","\n","    for i in range(int(img_height/2), img_height):\n","        for j in range(img_width):\n","          a = visualize_masks[k][i-1,j-1,:] == output[i-1,j-1,:]\n","          if sum(a.numpy())==3:\n","            intersection += 1\n","\n","    iou_score_half.append(round(intersection/union, ndigits=2))\n","\n","    plt.subplot(3, 4, k + 9)\n","    plt.imshow(output)\n","    # if k==0:\n","    #   plt.ylabel(\"Output\")\n","    plt.xlabel(\"IoU: %s\" % iou_score[k+4])\n","    plt.yticks(())\n","    plt.xticks(())\n","\n","  plt.savefig(path_result + \"A_NSTs_\" + str(num+1) + \"_2.png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","# Visualización final 3 (gráfico) &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  iou = []\n","  iou_half = []\n","  for i in range(len(iou_score)):\n","    if i != 4:\n","      iou.append(iou_score[i])\n","      iou_half.append(iou_score_half[i])\n","\n","  fig = px.Figure(data=[px.Scatter(name = \"Completa\", x = [0,50,75,100,200,500,1000], y = iou),\n","                        px.Scatter(name = \"Mitad\", x = [0,50,75,100,200,500,1000], y = iou_half)])\n","\n","  fig.update_layout(title=\"IoU respecto a la cantidad de estilo\",\n","                    xaxis_title=\"Cantidad de estilo\",\n","                    yaxis_title=\"IoU\",\n","                    autosize=False)\n","  fig.update_yaxes(range=[0, 1.05])\n","  fig.show()\n","  fig.write_image(path_result + \"A_comparacion_NST_\" + str(num+1) + \".png\")\n","\n","  print(\"Completado el\", str(num+1))"],"metadata":{"id":"zd0pLQUpzaMZ"},"execution_count":null,"outputs":[]}]}