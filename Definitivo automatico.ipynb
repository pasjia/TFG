{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["L6ejeVWQlC2r","WfDzaTLbfBL5","eV5ITTLefOeX","EwIv0X3rfRFn","Q8fKCOyBfbEL","NImACSydk5uQ"],"authorship_tag":"ABX9TyMy68CAtiXVHhrxRQTM1eAT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importación de librerias"],"metadata":{"id":"L6ejeVWQlC2r"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjmpMw3ibS2e","executionInfo":{"status":"ok","timestamp":1667559581982,"user_tz":-60,"elapsed":27527,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}},"outputId":"79a84d87-12ce-4b40-fe1b-bef7e14f638c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/tensorflow/examples.git\n","  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-5uzim4xl\n","  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-5uzim4xl\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===e2510e7de8354ea89c54ab376ce52371efb39eff-) (1.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===e2510e7de8354ea89c54ab376ce52371efb39eff-) (1.15.0)\n","Building wheels for collected packages: tensorflow-examples\n","  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-examples: filename=tensorflow_examples-e2510e7de8354ea89c54ab376ce52371efb39eff_-py3-none-any.whl size=299719 sha256=e39cb9aafd853e737067ddc6c15f447299e82276872a31d7c4fcaa719ab6b38e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qth2y9pz/wheels/eb/19/50/2a4363c831fa12b400af86325a6f26ade5d2cdc5b406d552ca\n","\u001b[33m  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but 'e2510e7de8354ea89c54ab376ce52371efb39eff-' is not\u001b[0m\n","Failed to build tensorflow-examples\n","Installing collected packages: tensorflow-examples\n","    Running setup.py install for tensorflow-examples ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n","Successfully installed tensorflow-examples-e2510e7de8354ea89c54ab376ce52371efb39eff-\n"]}],"source":["!pip install git+https://github.com/tensorflow/examples.git"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import os\n","import matplotlib.pyplot as plt\n","\n","import PIL.Image\n","\n","import cv2\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udexQExIbZYU","executionInfo":{"status":"ok","timestamp":1667559604865,"user_tz":-60,"elapsed":22887,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}},"outputId":"c087ff58-a4ec-42df-87ed-d861c7488210"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Creación de variables y fuciones"],"metadata":{"id":"Nx3vpg7TfFTT"}},{"cell_type":"markdown","source":["## Ajustes NST"],"metadata":{"id":"WfDzaTLbfBL5"}},{"cell_type":"code","source":["STYLE_WEIGHT = 100\n","CONTENT_WEIGHT = 0.5\n","Nepochs = 20000\n","muestra_cada = 200"],"metadata":{"id":"e2tA9dP9bZO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def vgg_layers(layer_names):\n","    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n","    # Load our model. Load pretrained VGG, trained on imagenet data\n","    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","    vgg.trainable = False\n","\n","    outputs = [vgg.get_layer(name).output for name in layer_names]\n","\n","    model = tf.keras.Model([vgg.input], outputs)\n","    return model\n","\n","\n","def gram_matrix(input_tensor):\n","    result = tf.math.reduce_mean(input_tensor, [1,2])\n","    return result#/(num_locations)\n","\n","\n","class StyleContentModel(tf.keras.models.Model):\n","    def __init__(self, style_layers, content_layers):\n","        super(StyleContentModel, self).__init__()\n","        self.vgg = vgg_layers(style_layers + content_layers)\n","        self.style_layers = style_layers\n","        self.content_layers = content_layers\n","        self.num_style_layers = len(style_layers)\n","        self.vgg.trainable = False\n","\n","    def call(self, inputs):\n","        \"Expects float input in [0,1]\"\n","        inputs = inputs*255.0\n","        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","        outputs = self.vgg(preprocessed_input)\n","        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n","                                          outputs[self.num_style_layers:])\n","\n","        style_outputs = [gram_matrix(style_output)\n","                         for style_output in style_outputs]\n","\n","        content_dict = {content_name: value\n","                        for content_name, value\n","                        in zip(self.content_layers, content_outputs)}\n","\n","        style_dict = {style_name: value\n","                      for style_name, value\n","                      in zip(self.style_layers, style_outputs)}\n","\n","        return {'content': content_dict, 'style': style_dict}\n","\n","\n","def style_content_loss(outputs,style_weight,content_weight):\n","    style_outputs = outputs['style']\n","    content_outputs = outputs['content']\n","\n","    #style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","    #                       for name in style_outputs.keys()])\n","    #style_loss *= style_weight \n","\n","    style_loss = [style_weight*tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","                           for name in style_outputs.keys()]\n","\n","    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n","                             for name in content_outputs.keys()])\n","    content_loss *= content_weight \n","    loss = style_loss + content_loss\n","    \n","    return loss, style_loss, content_loss\n","\n","\n","def clip_0_1(image):\n","      return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"],"metadata":{"id":"LO7xvOFCbZMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["opt = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.99, epsilon=1e-1)\n","\n","style_layers_ori = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1']\n","\n","content_layers = ['block5_conv2'] "],"metadata":{"id":"-pCN7Sv9bZKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ajustes GAN"],"metadata":{"id":"eV5ITTLefOeX"}},{"cell_type":"code","source":["def generator_gta():\n","    for path in images_GTA:\n","        img = cv2.imread(path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (286,286))\n","        yield img\n","\n","\n","def generator_cityscapes():\n","    for path in images_Reales:\n","        img = cv2.imread(path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (286,286))\n","        yield img\n","\n","\n","def preprocess_imgs(img):\n","    ## Convertimos las imágenes de int a float\n","    img = tf.cast(img, tf.float32)\n","\n","    ## Normalizamos al rango [-1,1]\n","    img = (img/127.5) - 1\n","\n","    ## Extraemos recortes aleatorios de 256x256\n","    img = tf.image.random_crop(img, size=[256, 256, 3])\n","\n","    return img\n","\n","\n","def renormalize(img, contrast=1):\n","    return img*0.5*contrast + 0.5"],"metadata":{"id":"h998DHyFbjAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_CHANNELS = 3\n","\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","\n","\n","generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","checkpoint_path = \"./Checkpoints/\"\n","\n","ckpt = tf.train.Checkpoint(generator_g=generator_g,\n","                           generator_f=generator_f,\n","                           discriminator_x=discriminator_x,\n","                           discriminator_y=discriminator_y,\n","                           generator_g_optimizer=generator_g_optimizer,\n","                           generator_f_optimizer=generator_f_optimizer,\n","                           discriminator_x_optimizer=discriminator_x_optimizer,\n","                           discriminator_y_optimizer=discriminator_y_optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"],"metadata":{"id":"RuHwlf0Mbi1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ajustes visualización"],"metadata":{"id":"EwIv0X3rfRFn"}},{"cell_type":"code","source":["def one_hot_mask(y):\n","    ''' Do the one hot encoding for the masks.\n","  \n","    Arguments:\n","        - y (tf tensor): Mask of shape (height, width, 3)\n","\n","    Returns:\n","        - mask (tf tensor): Mask after do the one hot. Shape (height, width, num_classes) '''\n","\n","    one_hot_map = []\n","    for color in colors:\n","        class_map = tf.reduce_all(tf.equal(y, color), axis = -1)\n","        one_hot_map.append(class_map)\n","    mask = tf.cast(tf.stack(one_hot_map, axis = -1), tf.int32)\n","    return mask\n","\n","\n","def load_image(folder, file, height = 96, width = 256, crop = False):\n","    ''' Load and preprocess a train image by:\n","        - Crop the image to not have the Mercedes-Benz star\n","        - Resize the image to (height, width)\n","        - Normalize the image to [0, 1]\n","  \n","    Arguments:\n","        - folder (string): Path to the folder\n","        - file (string): Name of the file to load\n","        - height (int): Height to resize -- 96\n","        - width (int): Width to resize -- 256\n","        - crop (bool): Crpo the image or not -- True\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed image '''\n","\n","    # Load the image (png)\n","    image = tf.io.read_file(folder + '/' + file)\n","    image = tf.cast(tf.image.decode_png(image, channels = 3), tf.float32)\n","\n","    # Crop the image\n","    if crop:\n","        image = tf.image.crop_to_bounding_box(image, 0, 0, 768, 2048)\n","\n","    # Resize the image\n","    image = tf.image.resize(image, (height, width))\n","\n","    # Normalize the image\n","    image = tf.cast(image, tf.float32)/255.0\n","    return image\n","\n","\n","def load_mask(folder, file, height = 96, width = 256, one_hot = True, crop = False):\n","    ''' Load and preprocess a train mask by:\n","        - Crop the image to not have the Mercedes-Benz star\n","        - Resize the image to (height, width)\n","        - Reshaping the mask from (height, width, 3) to (height, width, 30): One hot encoding\n","  \n","    Arguments:\n","        - folder (string): Path to the folder\n","        - file (string): Name of the file to load\n","        - height (int): Height to resize -- 96\n","        - width (int): Width to resize -- 256\n","        - one_hot (bool): Do one hot encoding or not -- True\n","        - crop (bool): Crpo the image or not -- True\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed mask '''\n","\n","    # Load the mask (png)\n","    image = tf.io.read_file(folder + '/' + file)\n","    image = tf.cast(tf.image.decode_png(image, channels = 3), tf.int32)\n","\n","    # Crop the mask\n","    if crop:\n","        image = tf.image.crop_to_bounding_box(image, 0, 0, 768, 2048)\n","\n","    # Resize the mask\n","    image = tf.image.resize(image, (height, width), method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","    # One hot encoding\n","    if one_hot:\n","        image = one_hot_mask(image)\n","    return image\n","\n","\n","def one_hot_to_color_mask(mask, colors, img_height = 96, img_width = 256):\n","    ''' Convert from the mask from the classes with highest probablities to the correct color. From (96, 256, 1) to (96, 256, 3).\n","  \n","    Arguments:\n","        - mask (tf tensor): Mask with the classes with highest probabilities\n","        - colors (list): List with the class colors\n","        - img_height (int): Height of the images -- 96\n","        - img_width (int): Width of the images -- 256\n","\n","    Returns:\n","        - color_mask (tf tensor): Color mask '''\n","\n","    color_mask = np.zeros((img_height, img_width, channels)).astype('float')\n","    for c in range(len(colors)):\n","        color_true = mask == c\n","        for i in range(3):\n","            color_mask[:,:,i] += color_true*colors[c][i]\n","\n","    color_mask = tf.cast(color_mask, dtype = tf.int32)\n","\n","    return color_mask\n","\n","def load_train(image_name, mask_name):\n","    ''' Load and preprocess a train image and its mask\n","  \n","    Arguments:\n","        - image_name (string): Name of the image to load\n","        - mask_name (string): Name of the mask to load\n","\n","    Returns:\n","        - image (tf tensor): Preprocessed image\n","        - mask (tf tensor): Preprocessed mask '''\n","\n","    image = load_image(train_images_folder_path, image_name, img_height, img_width)\n","    mask = load_mask(train_mask_folder_path, mask_name, img_height, img_width)\n","    return image, mask"],"metadata":{"id":"7HBMlEJbcBSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_images_folder_path = \"/content/drive/My Drive\" \n","train_mask_folder_path = \"/content/drive/MyDrive/GTA Segmentacion\" \n","\n","img_height, img_width, channels = 96, 256, 3\n","\n","batch_size = 2\n","\n","colors = np.array([(0, 0, 0), (111, 74, 0), (81, 0, 81), (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153), (180, 165, 180), \n","                   (150, 100, 100), (150, 120, 90), (153, 153, 153), (250, 170, 30), (220, 220, 0), (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), ( 0, 0, 142), \n","                   ( 0, 0, 70), (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)], dtype = np.int32)\n","\n","model = tf.keras.models.load_model('/content/drive/My Drive/Modelo_segmentacion_UNET/best_model_weights_and_architecture')\n"],"metadata":{"id":"ZljlfDWJgebd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# La magia"],"metadata":{"id":"boOe8wf_ceui"}},{"cell_type":"markdown","source":["## Inicialización"],"metadata":{"id":"Q8fKCOyBfbEL"}},{"cell_type":"code","source":["path_result = \"/content/drive/MyDrive/Resultados/\"\n","\n","path_GTA = \"/content/drive/MyDrive/GTA/\"\n","path_Reales = \"/content/drive/MyDrive/Reales/\"\n","# path_CARLA = \"/content/drive/MyDrive/CARLA/\"\n","\n","iter = 20  # Número de imagenes de prueba\n","\n","list_GTA = os.listdir(path_GTA)\n","images_GTA = random.choices(list_GTA, k=iter)\n","\n","# list_CARLA = os.listdir(path_CARLA)\n","# images_CARLA = random.choices(list_CARLA, k=iter)\n","\n","list_Reales = os.listdir(path_Reales)\n","images_Reales = random.choices(list_Reales, k=iter)\n","\n","for i in range(iter):\n","  images_GTA[i] = path_GTA + images_GTA[i]\n","  # images_CARLA[i] = path_CARLA + images_CARLA[i]\n","  images_Reales[i] = path_Reales + images_Reales[i]"],"metadata":{"id":"0GREBr28bZVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_gta = tf.data.Dataset.from_generator(\n","    generator_gta,\n","    output_signature=(\n","        tf.TensorSpec((286, 286, 3), tf.int32)\n","    )\n",")\n","\n","dataset_cityscapes = tf.data.Dataset.from_generator(\n","    generator_cityscapes,\n","    output_signature=(\n","        tf.TensorSpec((286, 286, 3), tf.int32)\n","    )\n",")\n","\n","dataset_gta_rdy = dataset_gta.map(preprocess_imgs)\n","dataset_cityscapes_rdy = dataset_cityscapes.map(preprocess_imgs)\n","\n","dataset_full = tf.data.Dataset.zip((dataset_gta_rdy, dataset_cityscapes_rdy))"],"metadata":{"id":"M8Kl4YDMbi5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Que fotos he escogido"],"metadata":{"id":"NImACSydk5uQ"}},{"cell_type":"code","source":["for name in images_GTA:\n","  print(name.split(\"/\")[-1])\n","print(\"------------------------------------------------\")\n","for name in images_Reales:\n","  print(name.split(\"/\")[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpH147INf0Vx","executionInfo":{"status":"ok","timestamp":1667559625877,"user_tz":-60,"elapsed":5,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}},"outputId":"7bbe6265-cc13-436f-a042-1e475f0af33f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["01657.png\n","01148.png\n","00952.png\n","02053.png\n","01994.png\n","00244.png\n","01461.png\n","01387.png\n","00431.png\n","00207.png\n","00009.png\n","00326.png\n","00698.png\n","00489.png\n","01103.png\n","00256.png\n","01907.png\n","01237.png\n","01910.png\n","01653.png\n","------------------------------------------------\n","cologne_000033_000019_leftImg8bit.png\n","darmstadt_000000_000019_leftImg8bit.png\n","bremen_000196_000019_leftImg8bit.png\n","monchengladbach_000000_010505_leftImg8bit.png\n","hanover_000000_045004_leftImg8bit.png\n","strasbourg_000001_010162_leftImg8bit.png\n","hanover_000000_032681_leftImg8bit.png\n","monchengladbach_000001_000876_leftImg8bit.png\n","dusseldorf_000010_000019_leftImg8bit.png\n","strasbourg_000000_019050_leftImg8bit.png\n","aachen_000103_000019_leftImg8bit.png\n","tubingen_000126_000019_leftImg8bit.png\n","bochum_000000_005936_leftImg8bit.png\n","bremen_000020_000019_leftImg8bit.png\n","strasbourg_000001_031683_leftImg8bit.png\n","hamburg_000000_037279_leftImg8bit.png\n","dusseldorf_000192_000019_leftImg8bit.png\n","hanover_000000_016038_leftImg8bit.png\n","aachen_000084_000019_leftImg8bit.png\n","strasbourg_000001_025426_leftImg8bit.png\n"]}]},{"cell_type":"markdown","source":["## Bucle"],"metadata":{"id":"OBZxzvSVk1c2"}},{"cell_type":"code","source":["for num in range(iter):\n","\n","  # NST  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  style_image = PIL.Image.open(images_Reales[num])\n","  content_image = PIL.Image.open(images_GTA[num])\n","  # content_image = PIL.Image.open(images_CARLA[num])\n","\n","  content_image = np.asarray(content_image.resize((img_width,img_width)))\n","  style_image = np.asarray(style_image.resize((img_width,img_width)))\n","\n","  content_image = content_image[:,:,0:3].astype(np.float32)/255\n","  style_image = style_image[:,:,0:3].astype(np.float32)/255\n","\n","  content_image = np.expand_dims(content_image,0)\n","  style_image = np.expand_dims(style_image,0)\n","\n","  for layer in np.arange(1,2):\n","\n","      style_layers = style_layers_ori[0:5]\n","\n","      extractor = StyleContentModel(style_layers, content_layers)\n","      style_targets = extractor(style_image)['style']\n","      content_targets = extractor(content_image)['content']\n","\n","      image = tf.Variable(content_image.copy())\n","\n","      # WEIGHTS FIRST GUESS\n","      style_weight_FG=5*1e-11\n","      content_weight_FG=1e-4\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(content_image),style_weight_FG,content_weight_FG)\n","      loss_style = np.array(style_loss[0])\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(style_image),style_weight_FG,content_weight_FG)\n","      loss_content = np.array(content_loss)\n","\n","      # WEIGTHS DEFINITION\n","      style_weight= style_weight_FG * STYLE_WEIGHT/loss_style\n","      content_weight= content_weight_FG * CONTENT_WEIGHT/loss_content\n","\n","      # Certify Weights\n","      loss, style_loss, content_loss = style_content_loss(extractor(content_image),style_weight,content_weight)\n","\n","      loss, style_loss, content_loss = style_content_loss(extractor(style_image),style_weight,content_weight)\n","\n","      @tf.function()\n","      def train_step(image):\n","          with tf.GradientTape() as tape:\n","              outputs = extractor(image)\n","              loss = style_content_loss(outputs,style_weight,content_weight)\n","\n","          grad = tape.gradient(loss[1][0], image)\n","          opt.apply_gradients([(grad, image)])\n","          image.assign(clip_0_1(image))\n","\n","      L_cont = np.array([])\n","      L_style = np.array([]) \n","\n","      for n in range(0,Nepochs):\n","          train_step(image)\n","          \n","          if np.mod(n,muestra_cada)==0:\n","              outputs = extractor(image)\n","              loss, style_loss, content_loss = style_content_loss(outputs,style_weight,content_weight)\n","              \n","              L_cont = np.append(L_cont,np.array(content_loss))\n","              L_style = np.append(L_style,np.array(style_loss[0]))\n","      \n","      Output_NST = image\n","\n","      ## Resultados\n","      plt.figure()\n","      plt.imshow(Output_NST[0, :,:,:],vmin=0,vmax=1), plt.axis('off')\n","      ## Guardamos el resultado\n","      plt.savefig(path_result + \"Output_NST_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","      \n","\n","  # GAN  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","\n","  i = 0\n","  for a, b in dataset_full.batch(1):\n","    if i==num:\n","      img_gta = a\n","      img_cs = b\n","    i += 1\n","\n","  ckpt.restore('/content/drive/MyDrive/CycleGTA/ckpt-6')\n","  Output_GAN_6 = generator_g.predict(img_gta)\n","\n","  ckpt.restore('/content/drive/MyDrive/CycleGTA/ckpt-10')\n","  Output_GAN_10 = generator_g.predict(img_gta)\n","\n","  ckpt.restore('/content/drive/MyDrive/CycleGTA/ckpt-12')\n","  Output_GAN_12 = generator_g.predict(img_gta)\n","\n","  ## Guardamos los resultados\n","  fig, axes = plt.subplots(1,1)\n","  plt.imshow(renormalize(Output_GAN_6.squeeze())), plt.axis('off')\n","  plt.savefig(path_result + \"Output_GAN_6_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","  fig, axes = plt.subplots(1,1)\n","  plt.imshow(renormalize(Output_GAN_10.squeeze())), plt.axis('off')\n","  plt.savefig(path_result + \"Output_GAN_10_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","  fig, axes = plt.subplots(1,1)\n","  plt.imshow(renormalize(Output_GAN_12.squeeze())), plt.axis('off')\n","  plt.savefig(path_result + \"Output_GAN_12_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","\n","  # Visualización final  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n","  images_name = [\"GTA/\" + images_GTA[num].split(\"/\")[-1], \"Resultados/Output_GAN_6_\" + str(num+1) + \".png\",\n","                 \"Resultados/Output_GAN_10_\" + str(num+1) + \".png\", \"Resultados/Output_GAN_12_\" + str(num+1) + \".png\",\n","                 \"Resultados/Output_NST_\" + str(num+1) + \".png\"]\n","\n","  mask_names = ((images_GTA[num].split(\"/\")[-1] + \" \") * 5).split()\n","\n","  dataset = tf.data.Dataset.from_tensor_slices((images_name, mask_names))\n","  dataset = dataset.map(load_train, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.batch(batch_size)\n","\n","\n","  predictions = model.predict(dataset, batch_size = 10)\n","  predictions = np.argmax(predictions, axis = 3)\n","\n","  idx = range(len(images_name))\n","  visualize_images = [load_image(train_images_folder_path, images_name[i], img_height, img_width) for i in idx]\n","  visualize_masks = [load_mask(train_mask_folder_path, mask_names[i], img_height, img_width, one_hot = False) for i in idx]\n","  preds = predictions[idx]\n","\n","\n","  # Resultados GTA\n","  plt.figure(figsize = (25, 5))\n","  plt.subplots_adjust(hspace = 0.1)\n","\n","  for k in range(5):\n","      plt.subplot(3, 5, k + 1)\n","      plt.imshow(visualize_images[k])\n","      if k==0:\n","        plt.ylabel(\"Input\")\n","      plt.title([\"Original\", \"CycleGAN-6\", \"CycleGAN-10\", \"CycleGAN-12\", \"NST\"][k])\n","      plt.yticks(())\n","      plt.xticks(())\n","\n","      plt.subplot(3, 5, k + 6)\n","      plt.imshow(visualize_masks[k])\n","      if k==0:\n","        plt.ylabel(\"Ground Truth\")\n","      plt.yticks(())\n","      plt.xticks(())\n","\n","      output = one_hot_to_color_mask(preds[k], colors)\n","      # IoU calculation\n","      intersection = 0\n","      union = int(img_height/2) * img_width\n","\n","      for i in range(int(img_height/2), img_height):  # Calcula de la mitad hacia abajo\n","        for j in range(img_width):\n","          a = visualize_masks[k][i-1,j-1,:] == output[i-1,j-1,:]\n","          if sum(a.numpy())==3:\n","            intersection += 1\n","\n","      iou_score = intersection/union\n","      \n","      plt.subplot(3, 5, k + 11)\n","      plt.imshow(output)\n","      if k==0:\n","        plt.ylabel(\"Output\")\n","      plt.xlabel(\"IoU: %s\" % iou_score)\n","      plt.yticks(())\n","      plt.xticks(())\n","\n","  plt.savefig(path_result + \"Output_GTA_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)\n","\n","  print(\"Completado el\", str(num+1))"],"metadata":{"id":"K3D4-nh7bZTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Resultados CARLA\n","# plt.figure(figsize = (25, 5))\n","# plt.subplots_adjust(hspace = 0.1)\n","\n","# for k in range(5):\n","#     plt.subplot(3, 5, k + 1)\n","#     plt.imshow(visualize_images[k+5])\n","#     if k==0:\n","#       plt.ylabel(\"Input\")\n","#     plt.title([\"Original\", \"CycleGAN-6\", \"CycleGAN-10\", \"CycleGAN-12\", \"NST\"][k])\n","#     plt.yticks(())\n","#     plt.xticks(())\n","\n","#     plt.subplot(3, 5, k + 6)\n","#     plt.imshow(visualize_masks[k+5])\n","#     if k==0:\n","#       plt.ylabel(\"Ground Truth\")\n","#     plt.yticks(())\n","#     plt.xticks(())\n","\n","#     output = one_hot_to_color_mask(preds[k+5], colors)\n","#     # IoU calculation\n","#     intersection = np.logical_and(visualize_masks[k+5], output)\n","#     union = np.logical_or(visualize_masks[k+5], output)\n","#     iou_score = np.sum(intersection) / np.sum(union)\n","\n","#     plt.subplot(3, 5, k + 11)\n","#     plt.imshow(output)\n","#     if k==0:\n","#       plt.ylabel(\"Output\")\n","#     plt.xlabel(\"IoU: %s\" % iou_score)\n","#     plt.yticks(())\n","#     plt.xticks(())\n","\n","# plt.savefig(path_result + \"Resultado_CARLA_\" + str(num+1) + \".png\", bbox_inches=\"tight\", pad_inches=0)"],"metadata":{"id":"BFXvzgcocK0L","executionInfo":{"status":"ok","timestamp":1667837302314,"user_tz":-60,"elapsed":4,"user":{"displayName":"alex Pastor","userId":"13585304210710441676"}}},"execution_count":1,"outputs":[]}]}